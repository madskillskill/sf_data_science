{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sber = pd.read_csv('data/sber_data.csv', sep=',')\n",
    "display(sber.head(10))\n",
    "display(sber.tail(10))\n",
    "display(sber.info())\n",
    "display(sber['sub_area'].describe())\n",
    "display(sber['price_doc'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_null = (sber.isnull().mean()*100).sort_values(ascending=False)\n",
    "display(col_null)\n",
    "col_null.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 4),\n",
    "    title='Распределение пропусков в данных'\n",
    ");\n",
    "colors = ['blue', 'yellow'] \n",
    "fig = plt.figure(figsize=(40, 4))\n",
    "cols = col_null.index\n",
    "ax = sns.heatmap(\n",
    "    sber[cols].isnull(),\n",
    "    cmap=sns.color_palette(colors),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_data = sber.copy()\n",
    "#задаем минимальный порог: вычисляем 70% от числа строк\n",
    "thresh = drop_data.shape[0]*0.7\n",
    "#удаляем столбцы, в которых более 30% (100-70) пропусков\n",
    "drop_data = drop_data.dropna(how='any', thresh=thresh, axis=1)\n",
    "#удаляем записи, в которых есть хотя бы 1 пропуск\n",
    "drop_data = drop_data.dropna(how='any', axis=0)\n",
    "#отображаем результирующую долю пропусков\n",
    "drop_data.isnull().mean()\n",
    "print(drop_data.shape)\n",
    "\n",
    "cols = col_null.index\n",
    "sber[cols].hist(figsize=(50, 25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем копию исходной таблицы\n",
    "fill_data = sber.copy()\n",
    "#создаем словарь имя столбца: число(признак) на который надо заменить пропуски\n",
    "values = {\n",
    "    'life_sq': fill_data['full_sq'],\n",
    "    'metro_min_walk': fill_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': fill_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': fill_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': fill_data['railroad_station_walk_min'].median(),\n",
    "    'hospital_beds_raion': fill_data['hospital_beds_raion'].mode()[0],\n",
    "    'preschool_quota': fill_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': fill_data['school_quota'].mode()[0],\n",
    "    'floor': fill_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем пропуски в соответствии с заявленным словарем\n",
    "fill_data = fill_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "fill_data.isnull().mean()\n",
    "\n",
    "cols = col_null.index\n",
    "fill_data[cols].hist(figsize=(50, 25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем копию исходной таблицы\n",
    "indicator_data = sber.copy()\n",
    "#в цикле пробегаемся по названиям столбцов с пропусками\n",
    "for col in col_null.index:\n",
    "    #создаем новый признак-индикатор как col_was_null\n",
    "    indicator_data[col + '_was_null'] = indicator_data[col].isnull()\n",
    "#создаем словарь имя столбца: число(признак) на который надо заменить пропуски   \n",
    "values = {\n",
    "    'life_sq': indicator_data['full_sq'],\n",
    "    'metro_min_walk': indicator_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': indicator_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': indicator_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': indicator_data['railroad_station_walk_min'].median(),\n",
    "    'hospital_beds_raion': indicator_data['hospital_beds_raion'].mode()[0],\n",
    "    'preschool_quota': indicator_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': indicator_data['school_quota'].mode()[0],\n",
    "    'floor': indicator_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем пропуски в соответствии с заявленным словарем\n",
    "indicator_data = indicator_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "indicator_data.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаём копию исходной таблицы\n",
    "combine_data = sber.copy()\n",
    "\n",
    "#отбрасываем столбцы с числом пропусков более 30% (100-70)\n",
    "n = combine_data.shape[0] #число строк в таблице\n",
    "thresh = n*0.7\n",
    "combine_data = combine_data.dropna(how='any', thresh=thresh, axis=1)\n",
    "\n",
    "#отбрасываем строки с числом пропусков более 2 в строке\n",
    "m = combine_data.shape[1] #число признаков после удаления столбцов\n",
    "combine_data = combine_data.dropna(how='any', thresh=m-2, axis=0)\n",
    "\n",
    "#создаём словарь 'имя_столбца': число (признак), на который надо заменить пропуски \n",
    "values = {\n",
    "    'life_sq': combine_data['full_sq'],\n",
    "    'metro_min_walk': combine_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': combine_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': combine_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': combine_data['railroad_station_walk_min'].median(),\n",
    "    'preschool_quota': combine_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': combine_data['school_quota'].mode()[0],\n",
    "    'floor': combine_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем оставшиеся записи константами в соответствии со словарем values\n",
    "combine_data = combine_data.fillna(values)\n",
    "#выводим результирующую долю пропусков\n",
    "display(combine_data.isnull().mean())\n",
    "\n",
    "print(combine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def outliers_iqr_mod(data, feature, log_scale=True, left=3, right=3):\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature])\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * left)\n",
    "    upper_bound = quartile_3 + (iqr * right)\n",
    "    \n",
    "    outliers = data[(x<lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x>lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "    \n",
    "\n",
    "sber_data = pd.read_csv('./data/sber_data.csv')\n",
    "outliers, cleaned = outliers_iqr_mod(sber_data, 'price_doc')\n",
    "\n",
    "outliers, cleaned = outliers_iqr_mod(sber_data, 'price_doc', left=3, right=3, log_scale=True)\n",
    "print(outliers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Давайте расширим правило 3ех сигм, чтобы иметь возможность учитывать особенности данных.\n",
    "    Добавьте в функцию outliers_z_score() параметры left и right, которые будут задавать число сигм (стандартных отклонений) \n",
    "    влево и вправо соответственно, которые определяют границы метода z-отклонения. \n",
    "    По умолчанию оба параметры равны 3\n",
    "\"\"\"\n",
    "    \n",
    "def outliers_z_score_mod(data, feature, log_scale=True,left=3.7,right=3.7):\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    mu = x.mean()\n",
    "    sigma = x.std()\n",
    "    lower_bound = mu - left * sigma\n",
    "    upper_bound = mu + right * sigma\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x > lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "    \n",
    "sber_data = pd.read_csv('./data/sber_data.csv')\n",
    "outliers, cleaned = outliers_z_score_mod(sber_data, 'price_doc', log_scale=True)\n",
    "print(outliers.shape[0])\n",
    "print(cleaned.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_information_cols = [] \n",
    "\n",
    "#цикл по всем столбцам\n",
    "for col in sber_data.columns:\n",
    "    #наибольшая относительная частота в признаке\n",
    "    top_freq = sber_data[col].value_counts(normalize=True).max()\n",
    "    #доля уникальных значений от размера признака\n",
    "    nunique_ratio = sber_data[col].nunique() / sber_data[col].count()\n",
    "    # сравниваем наибольшую частоту с порогом\n",
    "    if top_freq > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')\n",
    "    # сравниваем долю уникальных значений с порогом\n",
    "    if nunique_ratio > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes_data.csv')\n",
    "print(diabetes.head(20))\n",
    "\n",
    "dupl_columns = list(diabetes.columns)\n",
    "dupl_columns.remove('Gender')\n",
    "\n",
    "mask = diabetes.duplicated(subset=dupl_columns)\n",
    "diabetes_duplicates = diabetes[mask]\n",
    "print(f'Число найденных дубликатов: {diabetes_duplicates.shape[0]}')\n",
    "\n",
    "diabetes_dedupped = diabetes.drop_duplicates(subset=dupl_columns)\n",
    "print(f'Результирующее число записей: {diabetes_dedupped.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#список неинформативных признаков\n",
    "low_information_cols = [] \n",
    "\n",
    "#цикл по всем столбцам\n",
    "for col in diabetes_dedupped.columns:\n",
    "    #наибольшая относительная частота в признаке\n",
    "    top_freq = diabetes_dedupped[col].value_counts(normalize=True).max()\n",
    "    #доля уникальных значений от размера признака\n",
    "    nunique_ratio = diabetes_dedupped[col].nunique() / diabetes_dedupped[col].count()\n",
    "    # сравниваем наибольшую частоту с порогом\n",
    "    if top_freq > 0.99:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')\n",
    "    # сравниваем долю уникальных значений с порогом\n",
    "    if nunique_ratio > 0.99:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')\n",
    "        \n",
    "diabetes_dedupped.drop('Gender', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_null = (diabetes_dedupped.isnull().mean()*100).sort_values(ascending=False)\n",
    "display(col_null)\n",
    "col_null.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 4),\n",
    "    title='Распределение пропусков в данных'\n",
    ");\n",
    "\n",
    "\n",
    "colors = ['blue', 'yellow'] \n",
    "fig = plt.figure(figsize=(40, 4))\n",
    "cols = col_null.index\n",
    "ax = sns.heatmap(\n",
    "    diabetes_dedupped[cols].isnull(),\n",
    "    cmap=sns.color_palette(colors),\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_cutter(df, series):\n",
    "    count = 0\n",
    "    for i in series:\n",
    "        count +=0\n",
    "        for piece in df[[series]]:\n",
    "            if piece == 0:\n",
    "                df[i,piece] == np.nan\n",
    "            else:\n",
    "                pass\n",
    "    return None\n",
    "d_d = diabetes_dedupped.copy()\n",
    "print(type(d_d))\n",
    "\n",
    "#Glucose, BloodPressure, SkinThickness, Insulin и BMI\n",
    "\n",
    "d_d = zero_cutter(d_d, ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'])\n",
    "print(type(d_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
